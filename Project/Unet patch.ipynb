{"cells":[{"metadata":{},"cell_type":"markdown","source":"Initial setting."},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the sampling patch image information and batch size."},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 2\npatch_size = 256\nimage_size = patch_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Import the libraries needed."},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport os,sys,random,cv2,re\nimport tensorflow as tf\nimport tifffile\nfrom tensorflow import keras\n#from keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom scipy.ndimage.filters import gaussian_filter\nfrom tensorflow import reduce_sum\nfrom tensorflow.keras.backend import pow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\nfrom tensorflow.keras.losses import binary_crossentropy\n\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport glob\nimport skimage.io as io\nimport skimage.transform as trans\nimport matplotlib.pyplot as plt\nimport shutil\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\nfrom zipfile import ZipFile","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sampling patch for all the input image(output 270 pieces for both labels and images) "},{"metadata":{"trusted":true},"cell_type":"code","source":"imageDir = \"../input/images/\"\nlabelDir = \"../input/labels/\"\ndef patchDiv(imgPath,labelPath,patchSize=256):\n    #loop through image and label\n    images = os.listdir(imageDir)\n    labels = os.listdir(labelDir)\n    imagePatch = []\n    labelPatch = []\n    count = 1\n    for i in images:\n#         print(imgPath + i)\n        imageNumber = i.split(\".jpg\")[0][-2:]\n        imageLabel = \"/train-labels\" + imageNumber + \".jpg\"\n        \n        img = cv2.imread(imgPath + i,0)\n        label = cv2.imread(labelPath + imageLabel,0)\n        img = np.expand_dims(img,axis=2)\n        label = np.expand_dims(label,axis=2)\n        cutW = img.shape[0]//patchSize\n        cutH = img.shape[1]//patchSize\n        #divide image into patch\n        for w in range(0, (cutW * 10 // 2) + 5, 5):\n            for h in range(0, (cutH * 10 // 2) + 5, 5):\n                imagePatch.append(img[int((w/10)*patchSize):int(((w/10)+1)*patchSize), int((h/10)*patchSize):int(((h/10)+1)*patchSize)])\n                labelPatch.append(label[int((w/10)*patchSize):int(((w/10)+1)*patchSize), int((h/10)*patchSize):int(((h/10)+1)*patchSize)])\n    return imagePatch,labelPatch\nimgP,labeP = patchDiv(imageDir,labelDir,patchSize=patch_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Print out the first all of images and label that belong to first images and label.(For checking sampling patch is correct)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.figure(figsize=(100,100))\n# images = imgP[162:171]#os.listdir(imageDir)\n# labels = labeP[162:171]#os.listdir(labelDir)\n\n# for i in range(len(images)):\n#     plt.subplot(len(images),len(labels),i+1)\n#     plt.imshow(images[i][:,:,0],'gray')\n#     plt.subplot(len(images),len(labels),i+len(images)+1)\n#     plt.imshow(labels[i][:,:,0],'gray')\n# plt.show","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Split the data into train and validation.(Inorder to using validation when training the data)"},{"metadata":{"trusted":true},"cell_type":"code","source":"trainX,validX,trainY,validY = train_test_split(imgP,labeP,train_size=0.8, random_state = 2) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The data augmentation part.(All the patch that split by sampling patch)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#1. 数据增强 Data augmentation的参数\n\ndef saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n    stacked = []\n    for i,item in enumerate(npyfile):\n        img = item[:,:,0].astype('float32')\n        img = np.expand_dims(img,axis=0)\n        if(len(stacked)==0):\n            stacked = img\n        else:\n            stacked = np.vstack((stacked,img))\n    tifffile.imsave(save_path + '/allPredict.tif', stacked)\n\ndef adjustData(img,mask,flag_multi_class,num_class):\n    if(np.max(img) > 1):\n        #divided by 255 to convert to probability image\n        img = img / 255\n        mask = mask /255\n        mask[mask > 0.5] = 1\n        mask[mask <= 0.5] = 0\n    return (img,mask)\n\ndata_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='reflect')\n\ndef trainGenerator(batch_size,trainImage,trainLable,aug_dict,image_color_mode = \"grayscale\",\n                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (256,256),seed = 1):\n\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow(\n        x = np.array(trainImage),\n        batch_size = batch_size,\n        shuffle = True,\n        seed = seed,\n        save_to_dir = save_to_dir \n    )\n    mask_generator = mask_datagen.flow(\n        x = np.array(trainLable),\n        batch_size = batch_size,\n        shuffle = True,\n        seed = seed,\n        save_to_dir = save_to_dir \n    )\n\n    train_generator = zip(image_generator, mask_generator)\n    for (img,mask) in train_generator:\n        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n        yield (img,mask)\n        \nmyGene = trainGenerator(batch_size,trainX,trainY,data_gen_args,save_to_dir = None)\nmyValid = trainGenerator(batch_size,validX,validY,data_gen_args,save_to_dir = None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unet model."},{"metadata":{"trusted":true},"cell_type":"code","source":"def down_block(x,filters,kernel_size=(3,3),padding='same',strides=1):\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu')(x)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu')(c)\n    p=keras.layers.MaxPool2D((2,2),(2,2))(c)\n    return c,p\ndef up_block(x,skip,filters,kernel_size=(3,3),padding='same',strides=1):\n    us=keras.layers.UpSampling2D((2,2))(x)\n    concat=keras.layers.concatenate([us,skip],axis=3)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu')(concat)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu')(c)\n    return c\ndef bottleNeck(x,filters,kernel_size=(3,3),padding='same',strides=1):\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu')(x)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu')(c)\n    return c\ndef Unet():\n    f = [64,128,256,512,1024]#feature maps, number of filters\n    inputs=keras.layers.Input((image_size,image_size,1))#input to the model\n    p0 = inputs\n    c1,p1 = down_block(p0,f[0])#1->64\n    c2,p2 = down_block(p1,f[1])#64->128\n    c3,p3 = down_block(p2,f[2])#128->256\n    c4,p4 = down_block(p3,f[3])#256->512\n    drop1 = keras.layers.Dropout(0.5)(p4)\n    \n    bn = bottleNeck(drop1,f[4])#512->1024\n    drop2 = keras.layers.Dropout(0.5)(bn)\n\n    u1 = up_block(drop2,c4,f[3])#1024->512\n    u2 = up_block(u1,c3,f[2])#512->256\n    u3 = up_block(u2,c2,f[1])#256->128\n    u4 = up_block(u3,c1,f[0])#128->64\n    \n    outputs_up = keras.layers.Conv2D(2,(3,3),padding=\"same\",activation=\"relu\")(u4)#64->2\n    outputs = keras.layers.Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(outputs_up)#2->1\n    print(\"output\",outputs.shape)\n    model = keras.models.Model(inputs,outputs)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def Unet(pretrained_weights = None,input_size = (256, 256, 1)):\n#     inputs = keras.layers.Input(input_size)\n#     conv1 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n#     conv1 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n#     pool1 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n#     conv2 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n#     conv2 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n#     pool2 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n#     conv3 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n#     conv3 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n#     pool3 = keras.layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n#     conv4 = keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n#     conv4 = keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n#     drop4 = keras.layers.Dropout(0.5)(conv4)\n#     pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2))(drop4)\n\n#     conv5 = keras.layers.Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n#     conv5 = keras.layers.Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n#     drop5 = keras.layers.Dropout(0.5)(conv5)\n\n#     up6 = keras.layers.Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n#     merge6 = keras.layers.concatenate([drop4,up6], axis = 3)\n#     conv6 = keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n#     conv6 = keras.layers.Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n\n#     up7 = keras.layers.Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n#     merge7 = keras.layers.concatenate([conv3,up7], axis = 3)\n#     conv7 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n#     conv7 = keras.layers.Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n\n#     up8 = keras.layers.Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n#     merge8 = keras.layers.concatenate([conv2,up8], axis = 3)\n#     conv8 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n#     conv8 = keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n\n#     up9 = keras.layers.Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n#     merge9 = keras.layers.concatenate([conv1,up9], axis = 3)\n#     conv9 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n#     conv9 = keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n#     conv9 = keras.layers.Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n#     conv10 = keras.layers.Conv2D(1, 1, activation = 'sigmoid')(conv9)\n\n#     model = keras.models.Model(inputs,conv10)\n\n#     model.compile(optimizer = Adam(lr = 1e-4), loss = bce_dice_loss, metrics = ['accuracy', dice_coef])\n#     #model.compile(optimizer = Adam(lr = 1e-4), loss = focal_loss(gamma=2,alpha=0.6), metrics = ['accuracy', dice_coef])\n#     #model.compile(optimizer = Adam(lr = 1e-4), loss = focal_dice_loss, metrics = ['accuracy', dice_coef])\n    \n#     if(pretrained_weights):\n#         model.load_weights(pretrained_weights)\n\n#     return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loss function(Dice coefficeient and binary cross Dice coefficeient)"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred, smooth = 1):\n    return 1- dice_coef(y_true, y_pred, smooth)\n\ndef bce_dice_loss(y_true, y_pred, weight = 1, smooth = 1):\n    '''\n    bce + w*dice loss\n    '''\n    return binary_crossentropy(y_true, y_pred) + weight*dice_loss(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compile the model and training."},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Unet()\nopt = keras.optimizers.Adam(lr=1e-4)\nmodel.compile(optimizer=opt, loss=bce_dice_loss, metrics=[\"acc\"])#metrics=[dsc]\ntrainStep = len(trainX)//batch_size\nvalidStep = len(validX)//batch_size\nprint(trainStep)\nprint(validStep)\nhistory = model.fit_generator(myGene,steps_per_epoch=trainStep,epochs=80,validation_data=myValid,validation_steps=validStep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check out for the train loss and accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_final_history(history):\n    fig= plt.figure(figsize=(10,10))\n    plt.title('Loss')\n    plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    plt.legend()\n    fig= plt.figure(figsize=(10,10))\n    plt.title('Acc')\n    plt.plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    plt.legend()\n    fig= plt.figure(figsize=(10,10))\nshow_final_history(history)\nprint(\"Validation Accuracy: \" + str(history.history['val_acc'][-1:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Generate the testing data."},{"metadata":{"trusted":true},"cell_type":"code","source":"TestList = [\"../input/images/train-volume0%d.jpg\"%i if(i<10) else \"../input/images/train-volume%d.jpg\"%i for i in range(30)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def TestDataGenerator(imageList,patchSize=256):\n    for img in imageList:\n        x = cv2.imread(img,0)#256,256\n        x = np.expand_dims(x,axis=2)#256,256,1\n        x = np.expand_dims(x,axis=0)#1,256,256,1\n        num = x.shape[1]//patchSize # 2\n        \n        for i in range(0, (num * 10 // 2) + 5, 5):\n            for j in range(0, (num * 10 // 2) + 5, 5):\n                yield x[:,int((i/10)*patchSize):int(((i/10)+1)*patchSize),int((j/10)*patchSize):int(((j/10)+1)*patchSize)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testGene = TestDataGenerator(TestList)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The part that merge all the pieces that split by sampling patch into a completed image."},{"metadata":{"trusted":true},"cell_type":"code","source":"#merge the result \ndef mergeResult(imageList,originalImageShape=512,patchSize=256):\n    #index: number of elements in the input list for one image\n    cut = (originalImageShape//patchSize) + 1 #512 / 256 + 1 = 3\n    index = cut * cut\n    count = 0\n    i = 0\n    print(len(imageList))\n    mergedResult = np.zeros((len(imageList)//index, ) + (originalImageShape,originalImageShape,1))\n    while(count + index <= len(imageList)):#(9,18,27...) <= 270\n        vstacked = []\n        temp = imageList[count:count + index]#[0:9]\n        flag = 0\n        hstacked_list = []\n        for h in range(cut,index + 1, 3):#(2-4)\n            tmp = []\n            tmp2 = []\n            for r in range(flag,flag + cut):#(0-2),(2-4)\n                tmp.append(temp[r][:,:128,:])\n                tmp.append(temp[r][:,128:256,:])\n            tmp2.append(tmp[0])\n            tmp2.append((tmp[1] + tmp[2]) / 2)\n            tmp2.append((tmp[3] + tmp[4]) / 2)\n            tmp2.append(tmp[5])\n            \n            for k in range(len(tmp2)):\n                hstacked = tmp2[k] if(k == 0) else np.hstack((hstacked,tmp2[k]))\n            hstacked_list.append(hstacked)\n            flag = h\n            \n        vtmp = []\n        vtmp2 = []\n        for j in range(len(hstacked_list)):\n            vtmp.append(hstacked_list[j][:128,:,:])\n            vtmp.append(hstacked_list[j][128:256,:,:])\n        vtmp2.append(vtmp[0])\n        vtmp2.append((vtmp[1] + vtmp[2]) / 2)\n        vtmp2.append((vtmp[3] + vtmp[4]) / 2)\n        vtmp2.append(vtmp[5])\n        for l in range(len(vtmp2)):\n            vstacked = vtmp2[l] if (len(vstacked)==0) else np.vstack((vstacked,vtmp2[l]))\n        count += index\n        mergedResult[i] = vstacked\n        i += 1 \n    return mergedResult","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output and save the result."},{"metadata":{"trusted":true},"cell_type":"code","source":"myoutputDir = \"./output\"\nif not os.path.exists(myoutputDir):\n    os.makedirs(myoutputDir)\n    \nresult = model.predict_generator(testGene,steps=30*9,verbose=0)\nprint(\"output has %d images\"%len(result))\nprint(result[0].shape)\nresult = mergeResult(result)\nprint(\"final output after merging has %d images\"%len(result))\nplt.imshow(result[0][:,:,0],cmap=\"gray\")\nplt.show\nsaveResult(myoutputDir,result)\nwith ZipFile(\"./testOut.zip\",\"w\") as zip:\n    for file in os.listdir(myoutputDir):\n        zip.write(myoutputDir + \"/\" + file)\n        \nprint(list(zip.infolist()))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}