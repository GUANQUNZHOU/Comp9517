{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n#what can be done next?\n#1. exploit more on loss function\n#2. make unet deeper/wider?","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from __future__ import print_function\nimport os,sys,random,cv2,re,glob,shutil\nimport tensorflow as tf\nimport tifffile\nimport numpy as np\nimport skimage.io as io\nimport skimage.transform as trans\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.backend as K\n\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom scipy.ndimage.interpolation import map_coordinates\nfrom scipy.ndimage.filters import gaussian_filter\nfrom tensorflow import reduce_sum\nfrom tensorflow.keras.backend import pow\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPool2D, UpSampling2D, Concatenate, Add, Flatten\nfrom tensorflow.keras.losses import binary_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.wrappers.scikit_learn import KerasClassifier","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Hyper Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"Unet_model = False\nResUnet_mode = False\nimage_size = 512\nbatch_size = 2\nh,w = 512,512\nEpochs = 80\nimagePath = \"../input/images\"\nlabelPath = \"../input/labels\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train Test Split(80% trainng, 20%testing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#test-train split\nimageData = os.listdir(imagePath)\nimageLabel = os.listdir(labelPath)\n#ensure image and labels correspond\nimageData.sort()\nimageLabel.sort()\ntrainImage,validImage,trainLable,validLabel = train_test_split(imageData,imageLabel,test_size=0.2,shuffle=True,random_state=2)\ndef addPrefix(x):\n    pre = x.split(\"-\")[1][0:3]\n    if(pre == \"vol\"):\n        return imagePath + \"/\" + x\n    elif(pre == \"lab\"):\n        return labelPath + \"/\" + x\n    else:\n        return \"Invalid input\"\ntmp = [trainImage,validImage,trainLable,validLabel]\n\ntrainImage = list(map(addPrefix,trainImage))\nvalidImage = list(map(addPrefix,validImage))\ntrainLable = list(map(addPrefix,trainLable))\nvalidLabel = list(map(addPrefix,validLabel))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Image Augumentation"},{"metadata":{},"cell_type":"markdown","source":"Elastic Transform"},{"metadata":{"trusted":true},"cell_type":"code","source":"def elastic_transform(image, alpha, sigma, alpha_affine, random_state=None):\n    \"\"\"Elastic deformation of images as described in [Simard2003]_ (with modifications).\n    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis\", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n\n     Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n    \"\"\"\n    if random_state is None:\n        random_state = np.random.RandomState(None)\n    shape = image.shape\n    shape_size = shape[:2]\n    \n    # Random affine\n    center_square = np.float32(shape_size) // 2\n    square_size = min(shape_size) // 3\n    pts1 = np.float32([center_square + square_size, [center_square[0]+square_size, center_square[1]-square_size], center_square - square_size])\n    pts2 = pts1 + random_state.uniform(-alpha_affine, alpha_affine, size=pts1.shape).astype(np.float32)\n    M = cv2.getAffineTransform(pts1, pts2)\n    image = cv2.warpAffine(image, M, shape_size[::-1], borderMode=cv2.BORDER_REFLECT_101)\n\n    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n    dz = np.zeros_like(dx)\n\n    x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n    indices = np.reshape(y+dy, (-1, 1)), np.reshape(x+dx, (-1, 1)), np.reshape(z, (-1, 1))\n\n    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define function to draw a grid\ndef draw_grid(im, grid_size):\n    # Draw grid lines\n    for i in range(0, im.shape[1], grid_size):\n        cv2.line(im, (i, 0), (i, im.shape[0]), color=(255,))\n    for j in range(0, im.shape[0], grid_size):\n        cv2.line(im, (0, j), (im.shape[1], j), color=(255,))\n\n# Load images\nim = cv2.imread(imagePath + \"/train-volume00.jpg\", -1)\nim_mask = cv2.imread(labelPath + \"/train-labels00.jpg\", -1)\n# Draw grid lines\ndraw_grid(im, 50)\ndraw_grid(im_mask, 50)\n\n# Merge images into separete channels (shape will be (cols, rols, 2))\n\nim_merge = np.concatenate((im[...,None], im_mask[...,None]), axis=2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def elasticTrans(img, msk, batchSize = 2):\n    length = img.shape[1]\n    for i in range(img.shape[0]):\n        im_merge = np.concatenate((img[i], msk[i]), axis=2)\n        im_merge_t = elastic_transform(im_merge, im_merge.shape[1] * 2, \\\n                                       im_merge.shape[1] * 0.08,im_merge.shape[1] * 0.08)\n        img[i] = im_merge_t[..., 0].reshape(length, length, 1)\n        msk[i] = im_merge_t[..., 1].reshape(length, length, 1)\n    return img, msk","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Image after Elastic Transformation"},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\n# Apply transformation on image\nim_merge_t = elastic_transform(im_merge, im_merge.shape[1] * 2, im_merge.shape[1] * 0.08, im_merge.shape[1] * 0.08)\n# Split image and mask\nim_t = im_merge_t[...,0]\nim_mask_t = im_merge_t[...,1]\n# Display result\nplt.figure(figsize = (16,14))\nplt.imshow(np.c_[np.r_[im, im_mask], np.r_[im_t, im_mask_t]], cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def saveResult(save_path,npyfile,flag_multi_class = False,num_class = 2):\n    stacked = []\n    for i,item in enumerate(npyfile):\n        img = item[:,:,0].astype('float32')\n        img = np.expand_dims(img,axis=0)\n        if(len(stacked)==0):\n            stacked = img\n        else:\n            stacked = np.vstack((stacked,img))\n    tifffile.imsave(save_path + '/allPredict.tif', stacked)\n        #tifffile.imsave(, tiff_list)\n        #io.imsave(os.path.join(save_path,\"%d_predict.png\"%i),img)\n\ndef adjustData(img,mask,flag_multi_class,num_class):\n    if(np.max(img) > 1):\n        #divided by 255 to convert to probability image\n        img = img / 255\n        mask = mask /255\n        mask[mask > 0.5] = 1\n        mask[mask <= 0.5] = 0\n    return (img,mask)\n\ndata_gen_args = dict(rotation_range=0.2,\n                    width_shift_range=0.05,\n                    height_shift_range=0.05,\n                    shear_range=0.05,\n                    zoom_range=0.05,\n                    horizontal_flip=True,\n                    fill_mode='reflect')\n\ndef trainGenerator(batch_size,trainImage,trainLable,aug_dict,image_color_mode = \"grayscale\",\n                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (image_size,image_size),seed = 1):\n\n    image_datagen = ImageDataGenerator(**aug_dict)\n    mask_datagen = ImageDataGenerator(**aug_dict)\n    \n    image_generator = image_datagen.flow(\n        x = np.array([cv2.imread(ele,0).reshape((image_size,image_size,1)) for ele in trainImage]),\n        batch_size = batch_size,\n        shuffle = True,\n        seed = seed,\n        save_to_dir = save_to_dir \n    )\n    mask_generator = mask_datagen.flow(\n        x = np.array([cv2.imread(ele,0).reshape((image_size,image_size,1)) for ele in trainLable]),\n        batch_size = batch_size,\n        shuffle = True,\n        seed = seed,\n        save_to_dir = save_to_dir \n    )\n\n    train_generator = zip(image_generator, mask_generator)\n    for (img,mask) in train_generator:\n        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n        yield (img,mask)\n        \nmyGene = trainGenerator(batch_size,trainImage,trainLable,data_gen_args,save_to_dir = None)\nmyValid = trainGenerator(batch_size,validImage,validLabel,data_gen_args,save_to_dir = None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize data augumentation result"},{"metadata":{"trusted":true},"cell_type":"code","source":"a = myGene.__next__()\nk = 1\nplt.figure(figsize=(15,15))\nfor i in range(2):\n    for j in range(2):\n        plt.subplot(2,2,k)\n        plt.imshow(a[i][j].reshape(image_size,image_size),'gray')\n        k+=1\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Loss Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def dice_coef(y_true, y_pred, smooth=1):\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_loss(y_true, y_pred, smooth = 1):\n    return 1- dice_coef(y_true, y_pred, smooth)\n\ndef bce_dice_loss(y_true, y_pred, weight = 1, smooth = 1):\n    return binary_crossentropy(y_true, y_pred) + weight*dice_loss(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Unet Model "},{"metadata":{"trusted":true},"cell_type":"code","source":"def down_block(x,filters,kernel_size=(3,3),padding='same',strides=1):\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu',kernel_initializer = 'he_normal')(x)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu',kernel_initializer = 'he_normal')(c)\n    p=keras.layers.MaxPool2D((2,2),(2,2))(c)\n    return c,p\ndef up_block(x,skip,filters,kernel_size=(3,3),padding='same',strides=1):\n    us=keras.layers.UpSampling2D((2,2))(x)\n    concat=keras.layers.concatenate([us,skip],axis=3)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu',kernel_initializer = 'he_normal')(concat)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu',kernel_initializer = 'he_normal')(c)\n    return c\ndef bottleNeck(x,filters,kernel_size=(3,3),padding='same',strides=1):\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu',kernel_initializer = 'he_normal')(x)\n    c=keras.layers.Conv2D(filters,kernel_size,padding=padding,strides=strides,activation='relu',kernel_initializer = 'he_normal')(c)\n    return c\ndef Unet():\n    f = [64,128,256,512,1024]#feature maps, number of filters\n    inputs=keras.layers.Input((image_size,image_size,1))#input to the model\n    p0 = inputs\n    c1,p1 = down_block(p0,f[0])#1->64\n    c2,p2 = down_block(p1,f[1])#64->128\n    c3,p3 = down_block(p2,f[2])#128->256\n    c4,p4 = down_block(p3,f[3])#256->512\n    drop1 = keras.layers.Dropout(0.5)(p4)\n    \n    bn = bottleNeck(drop1,f[4])#512->1024\n    drop2 = keras.layers.Dropout(0.5)(bn)\n\n    u1 = up_block(drop2,c4,f[3])#1024->512\n    u2 = up_block(u1,c3,f[2])#512->256\n    u3 = up_block(u2,c2,f[1])#256->128\n    u4 = up_block(u3,c1,f[0])#128->64\n    \n    outputs_up = keras.layers.Conv2D(2,(3,3),padding=\"same\",activation=\"relu\",kernel_initializer = 'he_normal')(u4)#64->2\n    outputs = keras.layers.Conv2D(1,(1,1),padding=\"same\",activation=\"sigmoid\")(outputs_up)#2->1\n    model = keras.models.Model(inputs,outputs)\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ResUnet Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"def bn_act(x, act=True):\n    'batch normalization layer with an optinal activation layer'\n    x = tf.keras.layers.BatchNormalization()(x)\n    if act == True:\n        x = tf.keras.layers.Activation('relu')(x)\n    return x\ndef conv_block(x, filters, kernel_size=3, padding='same', strides=1):#BN + RELU + CONV\n    'convolutional layer which always uses the batch normalization layer'\n    conv = bn_act(x)\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(conv)\n    return conv\ndef stem(x, filters, kernel_size=3, padding='same', strides=1):\n    conv = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    conv = conv_block(conv, filters, kernel_size, padding, strides)\n    shortcut = Conv2D(filters, kernel_size=1, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([conv, shortcut])\n    return output\ndef residual_block(x, filters, kernel_size=3, padding='same', strides=1):\n    res = conv_block(x, filters, 3, padding, strides)\n    res = conv_block(res, filters, 3, padding, 1)\n    shortcut = Conv2D(filters, kernel_size, padding=padding, strides=strides)(x)\n    shortcut = bn_act(shortcut, act=False)\n    output = Add()([shortcut, res])\n    return output\ndef upsample_concat_block(x, xskip):\n    u = UpSampling2D((2,2))(x)\n    c = Concatenate()([u, xskip] )\n    return c\ndef ResUNet(img_size):\n    f = [16, 32, 64, 128, 256]\n    inputs = Input((img_size, img_size, 1))\n\n    ## Encoder\n    e0 = inputs\n    e1 = stem(e0, f[0])\n    e2 = residual_block(e1, f[1], strides=2)\n    e3 = residual_block(e2, f[2], strides=2)\n    e4 = residual_block(e3, f[3], strides=2)\n    e5 = residual_block(e4, f[4], strides=2)\n\n    ## Bridge\n    b0 = conv_block(e5, f[4], strides=1)\n    b1 = conv_block(b0, f[4], strides=1)\n\n    ## Decoder\n    u1 = upsample_concat_block(b1, e4)\n    d1 = residual_block(u1, f[4])\n\n    u2 = upsample_concat_block(d1, e3)\n    d2 = residual_block(u2, f[3])\n\n    u3 = upsample_concat_block(d2, e2)\n    d3 = residual_block(u3, f[2])\n\n    u4 = upsample_concat_block(d3, e1)\n    d4 = residual_block(u4, f[1])\n\n    outputs = tf.keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(d4)\n    model = tf.keras.models.Model(inputs, outputs)\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To use Unet model, uncomment the code below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Unet_model = Unet()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To use ResUnet model, uncomment the code below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"ResUnet_mode = ResUNet(image_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    if Unet_model:\n        model = Unet_model\n        print(\"using Unet Model\")\n    elif ResUnet_mode:\n        model = ResUnet_mode\n        print(\"using RestUnet Model\")\nexcept:\n    print(\"Error: To start, Select a specific model\")\n\nmodel.compile(optimizer = Adam(lr = 1e-4), loss = bce_dice_loss, metrics = ['accuracy', dice_coef])\nmodel_checkpoint = keras.callbacks.ModelCheckpoint('unet_membrane.hdf5', monitor='loss',verbose=1, save_best_only=True)\ntrainStep = len(trainImage)//batch_size\nvalidStep = len(validImage)//batch_size\nhistory = model.fit_generator(myGene,steps_per_epoch=trainStep,epochs=Epochs,callbacks=[model_checkpoint],\\\n                   validation_data = myValid, validation_steps = validStep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Show Learning Curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_final_history(history):\n    fig= plt.figure(figsize=(10,10))\n    plt.title('Loss')\n    plt.plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    plt.plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    plt.legend()\n    fig= plt.figure(figsize=(10,10))\n    plt.title('Acc')\n    plt.plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    plt.plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    plt.legend()\n    fig= plt.figure(figsize=(10,10))\n    plt.title('Dice_coef')\n    plt.plot(history.epoch, history.history[\"dice_coef\"], label=\"Train acc\")\n    plt.plot(history.epoch, history.history[\"val_dice_coef\"], label=\"Validation acc\")\n    plt.legend()\nshow_final_history(history)\nprint(\"Validation Accuracy: \" + str(history.history['val_acc'][-1:]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test Model and store outputs"},{"metadata":{"trusted":true},"cell_type":"code","source":"from zipfile import ZipFile\nimport cv2\n\ndirectory = os.listdir(\"../input/images/\")\ndirectory.sort()\n#create output folder\nif not os.path.exists(\"./output\"):\n    os.makedirs(\"./output\")\n\ndef testGenerator(test_path,target_size = (image_size,image_size)):\n    for i in test_path:\n        i = \"../input/images/\" + i\n        img = cv2.imread(i,0)\n        img = img / 255\n\n        img = trans.resize(img,target_size)\n        img = np.reshape(img,img.shape+(1,))\n        img = np.reshape(img,(1,)+img.shape)\n        yield img\n        \ntestGene = testGenerator(directory)\nresult = model.predict_generator(testGene,steps = len(directory),verbose=1)\n\nsaveResult(\"./output\",result)\nmyoutputDir = \"./output\"\n\nwith ZipFile(\"./testOut.zip\",\"w\") as zip:\n    for file in os.listdir(myoutputDir):\n        zip.write(\"./output/\" + file)\n        \nprint(list(zip.infolist()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}